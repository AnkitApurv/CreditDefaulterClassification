{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classifier: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.a. Import: Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#data organizing\n",
    "import pandas #storage\n",
    "import numpy as np #data-type conversion\n",
    "from os import getcwd\n",
    "\n",
    "#preprocessing - data splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#outlier removal to achieve better distribution\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import seaborn #test distribution\n",
    "\n",
    "#classifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow import feature_column\n",
    "import tensorflow as tf\n",
    "\n",
    "#classification result - statistical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/estimator/premade\n",
    "\n",
    "https://www.tensorflow.org/tutorials/structured_data/feature_columns\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier\n",
    "\n",
    "https://stackoverflow.com/questions/56612386/defining-the-input-function-for-tensorflow-pre-made-estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b. Import: Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#dtype changed from int64 to int32 to save space and speed up computation, no data was lost\n",
    "def cvDefPay(prediction):\n",
    "    mapper = {0: False, 1: True}\n",
    "    return mapper.get(prediction)\n",
    "\n",
    "url = getcwd() + '\\\\default of credit card clients.xls'\n",
    "ccd = pandas.read_excel(io = url, \\\n",
    "                        sheet_name='Data', header = 1, index_col = 0, \\\n",
    "                        dtype = {'LIMIT_BAL': np.int32, 'AGE': np.int32, 'BILL_AMT1': np.int32, 'BILL_AMT2': np.int32, 'BILL_AMT3': np.int32, 'BILL_AMT4': np.int32, 'BILL_AMT5': np.int32, 'BILL_AMT6': np.int32, 'PAY_AMT1': np.int32, 'PAY_AMT2': np.int32, 'PAY_AMT3': np.int32, 'PAY_AMT4': np.int32, 'PAY_AMT5': np.int32, 'PAY_AMT6': np.int32})\n",
    "                        #,converters = {'default payment next month': cvDefPay})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ccd.rename(columns = {'PAY_0': 'PAY_1'}, inplace = True)\n",
    "ccd.rename(columns = {'default payment next month': 'default_payment_next_month'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.a. Removing Outliers\n",
    "\n",
    "Since data is highly skewed with the higher end being very sparse, having mostly outliers,\n",
    "\n",
    "It may be better to remove those outliers so rest of the dataset has better distribution for better prediction\n",
    "And outlier datapoints could be have a separate classifier model\n",
    "\n",
    "Sould be done before data split to ensure distribution of train, dev and test sets are not different from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolationForest = IsolationForest(n_estimators = 100, max_samples = 0.2, contamination = 0.01,\n",
    "                       n_jobs = -1, random_state = 39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolationForest.fit(ccd)\n",
    "IsOutlierLabels = isolationForest.predict(ccd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvIsOutlier(prediction):\n",
    "    mapper = {-1: True, 1: False}\n",
    "    return mapper.get(prediction)\n",
    "\n",
    "ccdOutliers = ccd.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccdOutliers['IsOutlier'] = list(map(cvIsOutlier, IsOutlierLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inliers conditions have been selected from EDA observations\n",
    "\n",
    "ccdInliers = ccd[(ccdOutliers['IsOutlier'] == False) & (ccdOutliers['LIMIT_BAL'] <= 525000) & (ccdOutliers['AGE'] <= 60)]\n",
    "ccdOutliers = ccd[~ccd.index.isin(ccdInliers.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.jointplot(x = ccdOutliers['LIMIT_BAL'], y = ccdOutliers['AGE'], kind = 'kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.distplot(a = ccdOutliers['LIMIT_BAL'], bins = ccdOutliers['LIMIT_BAL'].value_counts().size, kde = True, hist = True, rug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.jointplot(x = ccdInliers['LIMIT_BAL'], y = ccdInliers['AGE'], kind = 'kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.distplot(a = ccdInliers['LIMIT_BAL'], bins = ccdInliers['LIMIT_BAL'].value_counts().size, kde = True, hist = True, rug = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "further inlier outlier correctness validation via pay_amt_mean, bill_amt_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.b.1. PAY {PAY_1 to PAY_6}\n",
    "\n",
    "1. Using mode to aggregate. An entry may have mutiple mode values (same frequency), to resolve, using severest class.\n",
    "\n",
    "2. Why severest value? To ensure fiscally fit population of credit users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ccdr = pandas.read_excel(io = url, \n",
    "                        sheet_name='Data', header = 1, index_col = 0)\n",
    "ccdr.rename(columns = {'PAY_0': 'PAY_1'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ccdrHistory = ccdr[['PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ccdrHistoryMode = ccdrHistory.mode(axis = 'columns')\n",
    "ccdrHistorySeverest = ccdrHistoryMode.apply(func = max, axis = 'columns')\n",
    "ccdPayHistoryMode = map(cvPayHistory, ccdrHistorySeverest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ccd['PAY_MODE_SEVEREST'] = list(ccdPayHistoryMode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.b.2. BILL_AMT {BILL_AMT1 to BILL_AMT6}\n",
    "\n",
    "Using mean for total credit used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ccdSpent = ccd[['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ccd['BILL_AMT_MEAN'] = np.int32(ccdSpent.mean(axis = 'columns').round())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.b.3. PAY_AMT {PAY_AMT1 to PAY_AMT6}\n",
    "\n",
    "Using mean for total credit settled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ccdSettled = ccd[['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ccd['PAY_AMT_MEAN'] = np.int32(ccdSettled.mean(axis = 'columns').round())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b.4. PAY_DELAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.c. Normalization\n",
    "\n",
    "Scaling: Only to reduce the effect of very large continuous variables (in distance based esimators).\n",
    "\n",
    "Normalization: Also reduce the effect of skewness in variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "varsToScale = ['LIMIT_BAL', 'AGE', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', \n",
    "               'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'BILL_AMT_MEAN', 'PAY_AMT_MEAN']\n",
    "scaler = StandardScaler(copy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in varsToScale:\n",
    "    ccd[var] = scaler.fit_transform(ccd[var].values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.d. Data Splitting\n",
    "\n",
    "Data is split before oversampling to avoid synthetic datapoints in test dataset.\n",
    "\n",
    "Test dataset is separated even though GridSearchCV uses Stratified K-Fold cross-validation so that model's accuracy can be tested independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccdY = pandas.DataFrame(ccd['default_payment_next_month'])\n",
    "ccdX = ccd.drop(['default_payment_next_month'], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(ccdX, ccdY, test_size = 0.25, stratify = ccdY, random_state = 44)\n",
    "\n",
    "trainX, validationX, trainY, validationY = train_test_split(trainX, trainY, test_size = 0.25, stratify = trainY, random_state = 44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_set = tf.data.Dataset.from_tensors((trainX.values, trainY.values))\n",
    "\n",
    "validation_set/dev_set = tf.data.Dataset.from_tensors((validationX.values, validationY.values))\n",
    "\n",
    "test_set = tf.data.Dataset.from_tensors((testX.values, testY.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fnx(features, labels, validation=True, batch_size=256):\n",
    "    train_data = tf.data.Dataset.from_tensors((trainX.values, trainY.values))\n",
    "    validation_data = tf.data.Dataset.from_tensors((validationX.values, validationY.values))\n",
    "    test_data = tf.data.Dataset.from_tensors((testX.values, testY.values))\n",
    "    #An input function for training or evaluating\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = train_data\n",
    "\n",
    "    # Shuffle and repeat if you are in training mode.\n",
    "    if validation:\n",
    "        dataset = validation_data\n",
    "    \n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_bal = feature_column.embedding_column('LIMIT_BAL', dimension = ccd['LIMIT_BAL'].nunique())\n",
    "\n",
    "sex1 = feature_column.categorical_column_with_vocabulary_list('SEX', [1, 2])\n",
    "sex = feature_column.indicator_column(sex1)\n",
    "\n",
    "education1 = feature_column.categorical_column_with_vocabulary_list('EDUCATION', [0, 1, 2, 3, 4, 5, 6])\n",
    "education = feature_column.indicator_column(education1)\n",
    "\n",
    "marriage1 = feature_column.categorical_column_with_vocabulary_list('MARRIAGE', [0, 1, 2, 3])\n",
    "marriage = feature_column.indicator_column(marriage1)\n",
    "\n",
    "age = feature_column.numeric_column('AGE')\n",
    "\n",
    "pay_11 = feature_column.categorical_column_with_vocabulary_list('PAY_1', [-2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "pay_1 = feature_column.indicator_column(pay_11)\n",
    "\n",
    "pay_21 = feature_column.categorical_column_with_vocabulary_list('PAY_2', [-2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "pay_2 = feature_column.indicator_column(pay_21)\n",
    "\n",
    "pay_31 = feature_column.categorical_column_with_vocabulary_list('PAY_3', [-2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "pay_3 = feature_column.indicator_column(pay_31)\n",
    "\n",
    "pay_41 = feature_column.categorical_column_with_vocabulary_list('PAY_4', [-2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "pay_4 = feature_column.indicator_column(pay_41)\n",
    "\n",
    "pay_51 = feature_column.categorical_column_with_vocabulary_list('PAY_5', [-2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "pay_5 = feature_column.indicator_column(pay_51)\n",
    "\n",
    "pay_61 = feature_column.categorical_column_with_vocabulary_list('PAY_6', [-2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "pay_6 = feature_column.indicator_column(pay_61)\n",
    "\n",
    "bill_amt1 = feature_column.numeric_column('BILL_AMT1')\n",
    "bill_amt2 = feature_column.numeric_column('BILL_AMT2')\n",
    "bill_amt3 = feature_column.numeric_column('BILL_AMT3')\n",
    "bill_amt4 = feature_column.numeric_column('BILL_AMT4')\n",
    "bill_amt5 = feature_column.numeric_column('BILL_AMT5')\n",
    "bill_amt6 = feature_column.numeric_column('BILL_AMT6')\n",
    "\n",
    "pay_amt1 = feature_column.numeric_column('PAY_AMT1')\n",
    "pay_amt2 = feature_column.numeric_column('PAY_AMT2')\n",
    "pay_amt3 = feature_column.numeric_column('PAY_AMT3')\n",
    "pay_amt4 = feature_column.numeric_column('PAY_AMT4')\n",
    "pay_amt5 = feature_column.numeric_column('PAY_AMT5')\n",
    "pay_amt6 = feature_column.numeric_column('PAY_AMT6')\n",
    "\n",
    "ccd_feature_columns = [limit_bal, sex, education, marriage, age,\n",
    "                   pay_1, pay_2, pay_3, pay_4, pay_5, pay_5,\n",
    "                   bill_amt1, bill_amt2, bill_amt3, bill_amt4, bill_amt5, bill_amt6,\n",
    "                   pay_amt1, pay_amt2, pay_amt3, pay_amt4, pay_amt5, pay_amt6]\n",
    "\n",
    "#initial_feature_count = 23\n",
    "#dimention_reduced_count = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = tf.estimator.DNNClassifier(feature_columns = ccd_feature_columns,\n",
    "                                         hidden_units = [23, 5],\n",
    "                                         n_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(input_fn = lambda: input_fnx(features = trainX, labels = trainY, validation = False), steps=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CreditDefaulterClassification",
   "language": "python",
   "name": "creditdefaulterclassification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
